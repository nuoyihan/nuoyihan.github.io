---
@article{objectagents,
  author = {Han, Violet Yinuo and Gonzalez, Jesse T. and Yang, Christina and Wang, Zhiruo and Hudson, Scott E. and Ion, Alexandra},
  title = {Towards Unobtrusive Physical AI: Augmenting Everyday Objects with Intelligence and Robotic Movement for Proactive Assistance},
  year = {2025},
  abstract = {Users constantly interact with physical, most often passive, objects. Consider if familiar objects instead proactively assisted users, e.g., a stapler moving across the table to help users organize documents, or a knife moving away to prevent injury as the user is inatten- tively about to lean against the countertop. In this paper, we build on the qualities of tangible interaction and focus on recognizing user needs in everyday tasks to enable ubiquitous yet unobtrusive tangible interaction. To achieve this, we introduce an architecture that leverages large language models (LLMs) to perceive users’ environment and activities, perform spatial-temporal reasoning, and generate object actions aligned with inferred user intentions and object properties. We demonstrate the system’s utility provid- ing proactive assistance with multiple objects and in various daily scenarios. To evaluate our system components, we compare our system-generated output for user goal estimation and object action recommendation with human-annotated baselines, with results indicating good agreement.},
  publisher = {Association for Computing Machinery},
  journal = {ACM UIST'25, Busan, Korea},
  month = {September},
  video_preview = {https://youtu.be/sWMiPVagiBs?si=nOSvy_ckka45_lrF},
  embed_videofigure = {https://www.youtube.com/embed/x3IRVDR3SjM?si=3GkbywvBaGSH5ksG},
  pdf = {https://interactive-structures.org/assets//publications/2025-09-object-agents/paper.pdf},
  doi = {10.1145/3746059.3747726},
  venue_link = {https://uist.acm.org/2025/},
  fig1 = {assets/img/objectagents.png},
  selected={true},
  preview = {objectagents.png},
  doi_host = {ACM Digital Library},
  venue = {ACM UIST 2025},
}

@article{Laymo,
  author = {Han, Violet Yinuo and Chen, Amber Yinglei and Zadan, Mason and Gonzalez, Jesse T. and Patel, Dinesh K. and Yu, Wendy Fangyu, and Majidi, Carmel and Ion, Alexandra},
  title = {Transforming Everyday Objects into Dynamic Interfaces using Smart Flat-Foldable Structures},
  year = {2025},
  abstract = {Dynamic physical interfaces are often dedicated devices designed to adapt their physical properties to user needs. In this paper, we present an actuation system that allows users to transform their existing objects into dynamic physical user interfaces. We design our actuation system to integrate as a self-contained locomotion layer into existing objects that are small-scale, i.e., hand-size rather than furniture-size. We envision that such objects can act as col- laborators: as a studio assistant in a painter’s palette, as tutors in a student’s ruler, or as caretakers for plants evading direct sunlight. The key idea is to decompose the actuation into (1) energy input and (2) steering to achieve a flat form factor. The energy input is provided by simple vibration. We implement steering through differential friction controlled by flat-foldable compliant structures that can be activated electrically. We study the mechanism and its performance, and show its application scenarios enabling dynamic interactions with objects.},
  publisher = {Association for Computing Machinery},
  journal = {ACM UIST'25, Busan, Korea},
  month = {September},
  video_preview = {https://youtu.be/l0rnG6rc5NA?si=NPZ8_q2ykTpmxi2o},
  embed_videofigure = {https://www.youtube.com/embed/XPE4Pmyh2fM?si=7V_t_Oi09GJEZd87},
  pdf = {https://interactive-structures.org/assets//publications/2025-09-laymo/paper.pdf},
  doi = {10.1145/3746059.3747720},
  venue_link = {https://uist.acm.org/2025/},
  fig1 = {assets/laymo-fig1.png},
  selected={false},
  preview = {laymo-placeholder.png},
  doi_host = {ACM Digital Library},
  venue = {ACM UIST 2025},
}

@article{DBN,
  author = {Han, Violet Yinuo and Wang, Tianyi and Cho, Hyunsung and Todi, Kashyap and Fernandes, Ajoy Savio and Levi, Andre, and Zhang, Zheng and Grossman, Tovi and Ion, Alexandra and Jonker, Tanya},
  title = {A Dynamic Bayesian Network Based Framework for Multimodal Context-Aware Interactions},
  year = {2025},
  abstract = {Multimodal context-aware interactions integrate multiple sensory inputs, such as gaze, gestures, speech, and environmental signals, to provide adaptive support across diverse user contexts. Building such systems is challenging due to the complexity of sensor fusion, real-time decision-making, and managing uncertainties from noisy inputs. To address these challenges, we propose a hybrid approach combining a dynamic Bayesian network (DBN) with a large language model (LLM). The DBN offers a probabilistic framework for modeling variables, relationships, and temporal dependencies, enabling robust, real-time inference of user intent, while the LLM incorporates world knowledge for contextual reasoning. We demonstrate our approach with a tri-level DBN implementation for tangible interactions, integrating gaze and hand actions to infer user intent in real time. A user evaluation with 10 participants in an everyday office scenario showed that our system can accurately and efficiently infer user intentions, achieving 0.83 per frame accuracy, even in complex environments. These results validate the effectiveness of the DBN+LLM framework for multimodal context-aware interactions.},
  publisher = {Association for Computing Machinery},
  journal = {ACM IUI'25, Cagliari, Italy},
  month = {March},
  video_preview = {https://youtu.be/rb8KfYchya8?si=qsFLddGYjNZqMdGw},
  embed_videofigure = {https://www.youtube.com/embed/rb8KfYchya8?si=THXqMyZrisI3v-vD},
  pdf = {https://interactive-structures.org/assets//publications/2025-03-dynamic-bayesian-network/paper.pdf},
  doi = {10.1145/3708359.3712070},
  venue_link = {https://iui.acm.org/2025/},
  fig1 = {assets/img/dbn-fig1.png},
  selected={true},
  preview = {dbn-placeholder.png},
  doi_host = {ACM Digital Library},
  venue = {ACM IUI 2025},
}
---
@article{rmm,
  author = {Cui, Zhitong and Wang, Shuhong and Han, Violet Yinuo and Rae-Grant, Tucker and Yang, Willa Yunqi and Zhu, Alan and Hudson, Scott E. and Ion, Alexandra},
  title = {Robotic Metamaterials: A Modular System for Hands-On Configuration of Ad-Hoc Dynamic Applications},
  year = {2024},
  abstract = {We propose augmenting initially passive structures built from simple repeated cells, with novel active units to enable dynamic, shape-changing, and robotic applications. Inspired by metamaterials that can employ mechanisms, we build a framework that allows users to configure cells of this passive structure to allow it to perform complex tasks. A key benefit is that our structures can be repeatedly (re)configured by users inserting our configuration units to turn the passive material into, e.g., locomotion robots, integrated motion platforms, or interactive interfaces, as we demonstrate in this paper.},
  publisher = {Association for Computing Machinery},
  journal = {ACM CHI'24, Honolulu, HI},
  month = {May},
  video_preview = {https://youtu.be/DmH4AIIZ42U?si=R5abVVefKTwhFeG6},
  embed_videofigure = {https://www.youtube.com/embed/8nzoPZUADJY?si=q8SVXYuthXAS0Ssl},
  pdf = {https://dl.acm.org/doi/pdf/10.1145/3613904.3642891},
  doi = {10.1145/3613904.3642891},
  venue_link = {https://chi2024.acm.org/},
  fig1 = {assets/img/blendmr-fig1.jpg},
  selected={false},
  preview = {rmm-preview.jpg},
  doi_host = {ACM Digital Library},
  venue = {ACM CHI 2024},
}

@article{blendmr,
  author = {Han, Violet Yinuo and Cho, Hyunsung and Maeda, Kiyosu and Ion, Alexandra and Lindlbauer, David},
  title = {BlendMR: A Computational Method to Create Ambient Mixed Reality Interfaces},
  year = {2023},
  abstract = {Mixed Reality (MR) systems display content freely in space, and present nearly arbitrary amounts of information, enabling ubiquitous access to digital information. This approach, however, introduces clutter and distraction if too much virtual content is shown. We present BlendMR, an optimization-based MR system that blends virtual content onto the physical objects in users’ environments for ambient information display. Our approach takes existing 2D applications and meshes of physical objects as input. It analyses the geometry of the physical objects and identifies regions that are suitable hosts for virtual elements. Using a novel integer programming formulation, our approach then optimally maps selected contents of the 2D applications onto the object, optimizing for factors such as importance and hierarchy of information, viewing angle, and geometric distortion. We evaluate BlendMR by comparing it to a 2D window baseline. Study results show that BlendMR decreases clutter and distraction, and is preferred by users. We demonstrate the applicability of BlendMR in a series of results and usage scenarios.},
  issue_date = {December 2023},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  volume = {7},
  number = {ISS},
  url = {https://doi.org/10.1145/3626472},
  doi = {10.1145/3626472},
  journal = {ACM ISS'23, Pittsburgh, PA},
  month = {nov},
  articleno = {436},
  numpages = {25},
  keywords = {augmented reality, computational interaction, adaptive user interfaces},
  selected={true},
  award = {Best Paper Award},
  video_preview = {https://youtu.be/3vQCdW7UCN8?si=50VJPeZ1Jm63qOiC},
  embed_videofigure = {https://www.youtube.com/embed/3vQCdW7UCN8?si=WHLPCVobPkGf785z},
  talk = {https://youtu.be/mO8k760D6oQ?si=8KK_6ueyyATOJiVV},
  embed_talk = {https://www.youtube.com/embed/mO8k760D6oQ?si=7O0j8T3gIDSYSHDw},
  preview = {blendmr-preview.png},
  pdf = {https://dl.acm.org/doi/pdf/10.1145/3626472},
  doi_host = {ACM Digital Library},
  venue = {ACM ISS 2023},
  venue_link = {https://iss2023.acm.org/},
  fig1 = {assets/img/blendmr-fig1.jpg}
}

@article{parametrichaptics,
  author = {Han, Violet Yinuo and Boadi-Agyemang, Abena and Lin, Yuyu and Lindlbauer, David and Ion, Alexandra},
  title = {Parametric Haptics: Versatile Geometry-Based Tactile Feedback Devices},
  year = {2023},
  abstract = {Haptic feedback is important for immersive, assistive, or multimodal interfaces, but engineering devices that generalize across applications is notoriously difficult. To address the issue of versatility, we propose Parametric Haptics, geometry-based tactile feedback devices that are customizable to render a variety of tactile sensations. To achieve this, we integrate the actuation mechanism with the tactor geometry into passive 3D printable patches, which are then connected to a generic wearable actuation interface consisting of micro gear motors. The key benefit of our approach is that the 3D-printed patches are modular, can consist of varying numbers and shapes of tactors, and that the tactors can be grouped and moved by our actuation geometry over large areas of the skin. The patches are soft, thin, conformable, and easy to customize to different use cases, thus potentially enabling a large design space of diverse tactile sensations. In our user study, we investigate the mapping between geometry parameters of our haptic patches and users’ tactile perceptions. Results indicate a good agreement between our parameters and the reported sensations, showing initial evidence that our haptic patches can produce a wide range of sensations for diverse use scenarios. We demonstrate the utility of our approach with wearable prototypes in immersive Virtual Reality (VR) scenarios, embedded into wearable objects such as glasses, and as wearable navigation and notification interfaces. We support designing such patches with a design tool in Rhino.},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/3586183.3606766},
  doi = {10.1145/3586183.3606766},
  journal = {ACM UIST'23, San Francisco, CA},
  articleno = {65},
  numpages = {13},
  keywords = {Fabrication, Programmable Matter, Metamaterials},
  location = {<conf-loc>, <city>San Francisco</city>, <state>CA</state>, <country>USA</country>, </conf-loc>},
  series = {UIST '23},
  selected={true},
  video_preview = {https://youtu.be/PIUCEdw4UqA?si=QcH5uaiibO217Xpx},
  embed_videofigure = {https://www.youtube.com/embed/PIUCEdw4UqA?si=wWJmv1Zw0UpA2Ulk},
  talk = {https://www.youtube.com/live/YzCC3NcGVrM?si=7Sl__0gcBOxCvi0V&t=15004},
  embed_talk = {https://www.youtube.com/embed/YzCC3NcGVrM?si=RUa_jscRkaLY18wE&amp;start=15022&amp;end=15776},
  preview = {parametrichaptics-preview.png},
  pdf = {https://dl.acm.org/doi/pdf/10.1145/3586183.3606766},
  doi_host = {ACM Digital Library},
  venue = {ACM UIST 2023},
  venue_link = {https://uist.acm.org/2023/},
  fig1 = {assets/img/parametrichaptics-fig1.jpg}
}


@ARTICLE{permeablesensors,
  author={de Souza Oliveira, Hugo and Khaanghah, Niloofar Saeedzadeh and Han, Violet Yinuo and Carrasco-Pena, Alejandro and Ion, Alexandra and Haller, Michael and Cantarella, Giuseppe and Münzenrieder, Niko},
  journal={IEEE Sensors Letters}, 
  title={Permeable Thermistor Temperature Sensors Based on Porous Melamine Foam}, 
  abstract = {Flexible sensors and electronics have gained much attention in recent years. They are especially interesting due to their abilities to conform to static and dynamic surfaces while keeping their functionality. These characteristics make them relevant for a wide range of applications, from health care and fitness monitoring to soft robotics. In this work, we go beyond simple mechanical flexibility and present a lightweight and permeable flexible sensor utilizing melamine foam as a substrate. The foam is coated with metallic copper (Cu) and semiconductive Indium-Gallium-Zinc-Oxide (InGaZnO) to form a thermistor-type temperature sensor. The sensor showed a very stable response when cycling the temperature between 25 °C and 51 °C, exhibiting a maximum sensitivity of −01.6%∘C−1 , a permeability of 366.6gm−2h−1 at 24 °C, and a maximum resistance variation of −2.9%RH−1 when varying the relative humidity from 40% to 70%. The device also remained fully functional even after being bent to a radius of 5 mm.},
  year={2023},
  volume={7},
  number={5},
  pages={1-4},
  doi={10.1109/LSENS.2023.3271590},
  doi_host = {IEEE Xplore},
  preview = {permeablesensors-preview.png},
  venue = {IEEE Sensors},
  fig1 = {assets/img/permeablesensors-fig1.png}
}
