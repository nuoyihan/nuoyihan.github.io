<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Violet Han</title> <meta name="author" content="Violet Han"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="violet han, violet yinuo han, yinuo han, hanyinuo"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/violet-favicon.png?6b472c93327e4b2d39c5455b7dcb5fca"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://www.violethan.com/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?e74e74bf055e5729d44a7d031a5ca6a5" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?6185d15ea1982787ad7f435576553d64"></script> <link rel="preconnect" href="https://fonts.googleapis.com"> <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin> <link href="https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&amp;display=swap" rel="stylesheet"> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Violet </span><span class="font-weight-bold">Han </span></a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">home<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv</a> </li> <li class="nav-item "> <a class="nav-link" href="/art/">art</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="container mt-5"> <div class="post"> <article> <div class="profile float-left"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/prof_pic-480.webp 480w, /assets/img/prof_pic-800.webp 800w, /assets/img/prof_pic-1400.webp 1400w, " sizes="(min-width: 950px) 276.0px, (min-width: 576px) 30vw, 95vw" type="image/webp"></source> <img src="/assets/img/prof_pic.jpg?2e7ff8bc1f7c85d13cde0f104508aa4e" class="img-fluid" width="100%" height="auto" alt="prof_pic.jpg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="clearfix"> <p><span style="font-size: 1.0em;font-weight:bolder;"> Hi! I’m <a href="https://nuoyihan.github.io" rel="external nofollow noopener" target="_blank">Violet</a>. I’m a 3rd year PhD student advised by <a href="http://alexandraion.com/" rel="external nofollow noopener" target="_blank">Professor Alexandra Ion</a> in <a href="https://interactive-structures.org/" rel="external nofollow noopener" target="_blank">Interactive Structures Lab</a>, at <a href="https://www.cmu.edu/" rel="external nofollow noopener" target="_blank">Carnegie Mellon University</a>’s <a href="https://hcii.cmu.edu/" rel="external nofollow noopener" target="_blank">Human-Computer Interaction Institute</a>.</span> I have also worked with <a href="https://www.davidlindlbauer.com/" rel="external nofollow noopener" target="_blank">Professor David Lindlbauer</a> in <a href="https://augmented-perception.org/" rel="external nofollow noopener" target="_blank">Augmented Perception Lab</a> as a Masters student.</p> <p><span style="font-size: 1.0em;font-weight:bolder;">Instead of adding robots to our world that automate tasks, what if our world itself can become robotic and intelligent? </span><span style="font-size: 1.0em;">My research explores <span style="font-weight:bolder;">intelligent, robotically augmented everyday environments</span> that <span style="font-weight:bolder;">enhance rather than replace human experiences.</span></span></p> <p><span style="font-size: 1.0em;">I build (1) physical systems that integrate into existing objects and environments, enabling multimodal input/output capabilities such as robotic manipulation, movement, and tactile display, and (2) digital systems that understand users and facilitate interactions through interaction intent inference and proactive assistance.</span></p> <p></p> <div class="containerr"> <span style="font-size: 1.0em;">Reach me at </span>   <span style="font-size: 1.3em;"> <a href="mailto:%79%69%6E%75%6F%68@%63%73.%63%6D%75.%65%64%75" title="email"><i class="fa-solid fa-envelope"></i></a>  <a href="https://scholar.google.com/citations?user=30fnypIAAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a>  <a href="https://github.com/nuoyihan" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-github"></i></a>  <a href="https://twitter.com/VioletHanyinuo" title="X" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-x-twitter"></i></a> </span>. <button class="moreabt" onclick="toggleText()" id="textButton"> More about me. </button> <span id="moreText" style="display:none;"> I enjoy running, painting, and hanging with my awesome friends and <a href="https://y0mingzhang.github.io/" rel="external nofollow noopener" target="_blank">Yiming</a>, among other things. </span> <button class="moreabt" onclick="toggleNews()" id="newsButton"> News. </button> <span id="newsText" style="display:none;"> <div style="display: inline-block; text-align: left;"> <ul-news> <li-news>Sept 2025</li-news> <li-news>Presenting 2 papers and a demo at UIST 2025, lezzgo. Excited to visit Busan and hang with old friends!</li-news> <li-news>July 2025</li-news> <li-news>My 3rd half marathon (Erie), first one with friends! Vimal, Taejun, and Andrew (Daehwa and Y cheered!)</li-news> <li-news>March 2025</li-news> <li-news>Virtually presented our DBN + LLM framework at IUI 2025, Cagliari, Italy</li-news> <li-news>Oct 2024</li-news> <li-news>Giving talk at Ellis High School, Pittsburgh</li-news> <li-news>May-Sept 2024</li-news> <li-news>Research Scientist Intern at Meta Reality Labs, Redmond</li-news> <li-news>May 2024</li-news> <li-news>Presenting Robotic Metamaterials at CHI 2024, Honolulu</li-news> <li-news>April 2024</li-news> <li-news>[Not my news but...] <a href="https://y0mingzhang.github.io/" rel="external nofollow noopener" target="_blank">Yiming</a> received an OpenAI fellowship. I am so very proud of my big head genius!</li-news> <li-news>Nov 2023</li-news> <li-news>Presented BlendMR at ISS, won best paper, Pittsburgh</li-news> <li-news>Nov 2023</li-news> <li-news>Presented Parametric Haptics at UIST, San Francisco</li-news> <li-news>August 2023</li-news> <li-news>Started My PhD with alex @ISL @CMU HCII, yay!</li-news> </ul-news> </div> </span> <script>function toggleNews(){let e=document.getElementById("newsText"),n=document.getElementById("newsButton");"inline"==e.style.display?(e.style.display="none",n.innerHTML="News."):(e.style.display="inline",n.innerHTML="News:")}document.addEventListener("touchstart",function(){},!0);</script> <script>function toggleText(){let e=document.getElementById("moreText"),t=document.getElementById("textButton");"inline"==e.style.display?(e.style.display="none",t.innerHTML="More about me."):(e.style.display="inline",t.innerHTML="More about me:")}</script> </div> </div> <br> <h2><a href="/publications/" style="color: inherit;">selected publications</a></h2> <div class="publications"> <ol class="bibliography"> <li> <div class="publication-row"> <div class="publication-responsive-2cols"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/publication_preview/objectagents-480.webp 480w, /assets/img/publication_preview/objectagents-800.webp 800w, /assets/img/publication_preview/objectagents-1400.webp 1400w, " sizes="800px" type="image/webp"></source> <img src="/assets/img/publication_preview/objectagents.png" class="preview" width="100%" height="auto" alt="objectagents.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="publication-responsive-2cols"> <div id="objectagents"> <div class="title"><a href="https://www.violethan.com/publications/objectagents/">Towards Unobtrusive Physical AI: Augmenting Everyday Objects with Intelligence and Robotic Movement for Proactive Assistance</a></div> <div class="author"> <em>Violet Yinuo Han</em>, Jesse T. Gonzalez, Christina Yang, Zhiruo Wang, Scott E. Hudson, and Alexandra Ion</div> <div class="periodical"> <em>ACM UIST’25, Busan, Korea</em>, Sep 2025 </div> <div class="periodical"> </div> <br> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="https://doi.org/10.1145/3746059.3747726" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a href="https://interactive-structures.org/assets//publications/2025-09-object-agents/paper.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://youtu.be/sWMiPVagiBs?si=nOSvy_ckka45_lrF" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> </div> <div class="abstract hidden"> <p>Users constantly interact with physical, most often passive, objects. Consider if familiar objects instead proactively assisted users, e.g., a stapler moving across the table to help users organize documents, or a knife moving away to prevent injury as the user is inatten- tively about to lean against the countertop. In this paper, we build on the qualities of tangible interaction and focus on recognizing user needs in everyday tasks to enable ubiquitous yet unobtrusive tangible interaction. To achieve this, we introduce an architecture that leverages large language models (LLMs) to perceive users’ environment and activities, perform spatial-temporal reasoning, and generate object actions aligned with inferred user intentions and object properties. We demonstrate the system’s utility provid- ing proactive assistance with multiple objects and in various daily scenarios. To evaluate our system components, we compare our system-generated output for user goal estimation and object action recommendation with human-annotated baselines, with results indicating good agreement.</p> </div> </div> </div> </div> </li> <li> <div class="publication-row"> <div class="publication-responsive-2cols"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/publication_preview/dbn-placeholder-480.webp 480w, /assets/img/publication_preview/dbn-placeholder-800.webp 800w, /assets/img/publication_preview/dbn-placeholder-1400.webp 1400w, " sizes="800px" type="image/webp"></source> <img src="/assets/img/publication_preview/dbn-placeholder.png" class="preview" width="100%" height="auto" alt="dbn-placeholder.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="publication-responsive-2cols"> <div id="DBN"> <div class="title"><a href="https://www.violethan.com/publications/DBN/">A Dynamic Bayesian Network Based Framework for Multimodal Context-Aware Interactions</a></div> <div class="author"> <em>Violet Yinuo Han</em>, Tianyi Wang, Hyunsung Cho, Kashyap Todi, Ajoy Savio Fernandes, Andre Levi, Zheng Zhang, Tovi Grossman, Alexandra Ion, and Tanya Jonker</div> <div class="periodical"> <em>ACM IUI’25, Cagliari, Italy</em>, Mar 2025 </div> <div class="periodical"> </div> <br> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="https://doi.org/10.1145/3708359.3712070" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a href="https://interactive-structures.org/assets//publications/2025-03-dynamic-bayesian-network/paper.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://youtu.be/rb8KfYchya8?si=qsFLddGYjNZqMdGw" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> </div> <div class="abstract hidden"> <p>Multimodal context-aware interactions integrate multiple sensory inputs, such as gaze, gestures, speech, and environmental signals, to provide adaptive support across diverse user contexts. Building such systems is challenging due to the complexity of sensor fusion, real-time decision-making, and managing uncertainties from noisy inputs. To address these challenges, we propose a hybrid approach combining a dynamic Bayesian network (DBN) with a large language model (LLM). The DBN offers a probabilistic framework for modeling variables, relationships, and temporal dependencies, enabling robust, real-time inference of user intent, while the LLM incorporates world knowledge for contextual reasoning. We demonstrate our approach with a tri-level DBN implementation for tangible interactions, integrating gaze and hand actions to infer user intent in real time. A user evaluation with 10 participants in an everyday office scenario showed that our system can accurately and efficiently infer user intentions, achieving 0.83 per frame accuracy, even in complex environments. These results validate the effectiveness of the DBN+LLM framework for multimodal context-aware interactions.</p> </div> </div> </div> </div> </li> <li> <div class="publication-row"> <div class="publication-responsive-2cols"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/publication_preview/blendmr-preview-480.webp 480w, /assets/img/publication_preview/blendmr-preview-800.webp 800w, /assets/img/publication_preview/blendmr-preview-1400.webp 1400w, " sizes="800px" type="image/webp"></source> <img src="/assets/img/publication_preview/blendmr-preview.png" class="preview" width="100%" height="auto" alt="blendmr-preview.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="publication-responsive-2cols"> <div id="blendmr"> <div class="title"><a href="https://www.violethan.com/publications/blendmr/">BlendMR: A Computational Method to Create Ambient Mixed Reality Interfaces</a></div> <div class="award"> <i class="fa-solid fa-trophy"></i> Best Paper Award</div> <div class="author"> <em>Violet Yinuo Han</em>, Hyunsung Cho, Kiyosu Maeda, Alexandra Ion, and David Lindlbauer</div> <div class="periodical"> <em>ACM ISS’23, Pittsburgh, PA</em>, Nov 2023 </div> <div class="periodical"> </div> <br> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="https://doi.org/10.1145/3626472" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a href="https://dl.acm.org/doi/pdf/10.1145/3626472" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://youtu.be/3vQCdW7UCN8?si=50VJPeZ1Jm63qOiC" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="https://youtu.be/mO8k760D6oQ?si=8KK_6ueyyATOJiVV" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Talk</a> </div> <div class="abstract hidden"> <p>Mixed Reality (MR) systems display content freely in space, and present nearly arbitrary amounts of information, enabling ubiquitous access to digital information. This approach, however, introduces clutter and distraction if too much virtual content is shown. We present BlendMR, an optimization-based MR system that blends virtual content onto the physical objects in users’ environments for ambient information display. Our approach takes existing 2D applications and meshes of physical objects as input. It analyses the geometry of the physical objects and identifies regions that are suitable hosts for virtual elements. Using a novel integer programming formulation, our approach then optimally maps selected contents of the 2D applications onto the object, optimizing for factors such as importance and hierarchy of information, viewing angle, and geometric distortion. We evaluate BlendMR by comparing it to a 2D window baseline. Study results show that BlendMR decreases clutter and distraction, and is preferred by users. We demonstrate the applicability of BlendMR in a series of results and usage scenarios.</p> </div> </div> </div> </div> </li> <li> <div class="publication-row"> <div class="publication-responsive-2cols"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/publication_preview/parametrichaptics-preview-480.webp 480w, /assets/img/publication_preview/parametrichaptics-preview-800.webp 800w, /assets/img/publication_preview/parametrichaptics-preview-1400.webp 1400w, " sizes="800px" type="image/webp"></source> <img src="/assets/img/publication_preview/parametrichaptics-preview.png" class="preview" width="100%" height="auto" alt="parametrichaptics-preview.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="publication-responsive-2cols"> <div id="parametrichaptics"> <div class="title"><a href="https://www.violethan.com/publications/parametrichaptics/">Parametric Haptics: Versatile Geometry-Based Tactile Feedback Devices</a></div> <div class="author"> <em>Violet Yinuo Han</em>, Abena Boadi-Agyemang, Yuyu Lin, David Lindlbauer, and Alexandra Ion</div> <div class="periodical"> <em>ACM UIST’23, San Francisco, CA</em>, Nov 2023 </div> <div class="periodical"> </div> <br> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="https://doi.org/10.1145/3586183.3606766" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a href="https://dl.acm.org/doi/pdf/10.1145/3586183.3606766" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://youtu.be/PIUCEdw4UqA?si=QcH5uaiibO217Xpx" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="https://www.youtube.com/live/YzCC3NcGVrM?si=7Sl__0gcBOxCvi0V&amp;t=15004" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Talk</a> </div> <div class="abstract hidden"> <p>Haptic feedback is important for immersive, assistive, or multimodal interfaces, but engineering devices that generalize across applications is notoriously difficult. To address the issue of versatility, we propose Parametric Haptics, geometry-based tactile feedback devices that are customizable to render a variety of tactile sensations. To achieve this, we integrate the actuation mechanism with the tactor geometry into passive 3D printable patches, which are then connected to a generic wearable actuation interface consisting of micro gear motors. The key benefit of our approach is that the 3D-printed patches are modular, can consist of varying numbers and shapes of tactors, and that the tactors can be grouped and moved by our actuation geometry over large areas of the skin. The patches are soft, thin, conformable, and easy to customize to different use cases, thus potentially enabling a large design space of diverse tactile sensations. In our user study, we investigate the mapping between geometry parameters of our haptic patches and users’ tactile perceptions. Results indicate a good agreement between our parameters and the reported sensations, showing initial evidence that our haptic patches can produce a wide range of sensations for diverse use scenarios. We demonstrate the utility of our approach with wearable prototypes in immersive Virtual Reality (VR) scenarios, embedded into wearable objects such as glasses, and as wearable navigation and notification interfaces. We support designing such patches with a design tool in Rhino.</p> </div> </div> </div> </div> </li> </ol> </div> </article> </div> </div> <footer class="sticky-bottom mt-5"> <div class="container"> © Copyright 2025 Violet Han. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a>, based on <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?9b43d6e67ddc7c0855b1478ee4c48c2d" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> </body> <head> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Violet Han</title> <meta name="author" content="Violet Han"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="violet han, violet yinuo han, yinuo han, hanyinuo"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/violet-favicon.png?6b472c93327e4b2d39c5455b7dcb5fca"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://www.violethan.com/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?e74e74bf055e5729d44a7d031a5ca6a5" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?6185d15ea1982787ad7f435576553d64"></script> <link rel="preconnect" href="https://fonts.googleapis.com"> <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin> <link href="https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&amp;display=swap" rel="stylesheet"> </head> </html>